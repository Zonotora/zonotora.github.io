(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[216],{4478:function(e,n,s){(window.__NEXT_P=window.__NEXT_P||[]).push(["/gpu",function(){return s(3625)}])},9427:function(e,n,s){"use strict";s.d(n,{R2:function(){return a},_4:function(){return l},_X:function(){return t},bW:function(){return c},nr:function(){return r}});var i=s(5893);let t=(0,i.jsx)("svg",{style:{position:"relative",right:"5px",top:" 2px"},xmlns:"http://www.w3.org/2000/svg",height:"1em",viewBox:"0 0 512 512",children:(0,i.jsx)("path",{d:"M320 0c-17.7 0-32 14.3-32 32s14.3 32 32 32h82.7L201.4 265.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L448 109.3V192c0 17.7 14.3 32 32 32s32-14.3 32-32V32c0-17.7-14.3-32-32-32H320zM80 32C35.8 32 0 67.8 0 112V432c0 44.2 35.8 80 80 80H400c44.2 0 80-35.8 80-80V320c0-17.7-14.3-32-32-32s-32 14.3-32 32V432c0 8.8-7.2 16-16 16H80c-8.8 0-16-7.2-16-16V112c0-8.8 7.2-16 16-16H192c17.7 0 32-14.3 32-32s-14.3-32-32-32H80z"})}),c=(0,i.jsx)("svg",{style:{position:"relative",right:"10px",top:" 2px"},xmlns:"http://www.w3.org/2000/svg",height:"1em",viewBox:"0 0 496 512",children:(0,i.jsx)("path",{d:"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"})}),r=(0,i.jsx)("svg",{fill:"#000000",height:"27px",viewBox:"1 -1 20 20",xmlns:"http://www.w3.org/2000/svg",children:(0,i.jsx)("path",{d:"M11.16 16.153a.477.477 0 0 1-.476.475H1.316a.477.477 0 0 1-.475-.475V3.046a.477.477 0 0 1 .475-.475h6.95l2.893 2.893zm-1.11-9.924H8.059a.575.575 0 0 1-.574-.574V3.679H1.95v11.84h8.102zM3.907 4.92a1.03 1.03 0 1 0 1.029 1.03 1.03 1.03 0 0 0-1.03-1.03zm4.958 3.253h-5.87v1.108h5.87zm0 2.354h-5.87v1.109h5.87zm0 2.354h-5.87v1.109h5.87z"})}),a=(0,i.jsx)("svg",{style:{position:"relative",top:"2px"},xmlns:"http://www.w3.org/2000/svg",height:"1em",viewBox:"0 0 384 512",children:(0,i.jsx)("path",{d:"M272 384c9.6-31.9 29.5-59.1 49.2-86.2l0 0c5.2-7.1 10.4-14.2 15.4-21.4c19.8-28.5 31.4-63 31.4-100.3C368 78.8 289.2 0 192 0S16 78.8 16 176c0 37.3 11.6 71.9 31.4 100.3c5 7.2 10.2 14.3 15.4 21.4l0 0c19.8 27.1 39.7 54.4 49.2 86.2H272zM192 512c44.2 0 80-35.8 80-80V416H112v16c0 44.2 35.8 80 80 80zM112 176c0 8.8-7.2 16-16 16s-16-7.2-16-16c0-61.9 50.1-112 112-112c8.8 0 16 7.2 16 16s-7.2 16-16 16c-44.2 0-80 35.8-80 80z"})}),l=(0,i.jsx)("svg",{style:{position:"relative",top:"2px"},xmlns:"http://www.w3.org/2000/svg",height:"1em",viewBox:"0 0 384 512",children:(0,i.jsx)("path",{d:"M297.2 248.9C311.6 228.3 320 203.2 320 176c0-70.7-57.3-128-128-128S64 105.3 64 176c0 27.2 8.4 52.3 22.8 72.9c3.7 5.3 8.1 11.3 12.8 17.7l0 0c12.9 17.7 28.3 38.9 39.8 59.8c10.4 19 15.7 38.8 18.3 57.5H109c-2.2-12-5.9-23.7-11.8-34.5c-9.9-18-22.2-34.9-34.5-51.8l0 0 0 0c-5.2-7.1-10.4-14.2-15.4-21.4C27.6 247.9 16 213.3 16 176C16 78.8 94.8 0 192 0s176 78.8 176 176c0 37.3-11.6 71.9-31.4 100.3c-5 7.2-10.2 14.3-15.4 21.4l0 0 0 0c-12.3 16.8-24.6 33.7-34.5 51.8c-5.9 10.8-9.6 22.5-11.8 34.5H226.4c2.6-18.7 7.9-38.6 18.3-57.5c11.5-20.9 26.9-42.1 39.8-59.8l0 0 0 0 0 0c4.7-6.4 9-12.4 12.7-17.7zM192 128c-26.5 0-48 21.5-48 48c0 8.8-7.2 16-16 16s-16-7.2-16-16c0-44.2 35.8-80 80-80c8.8 0 16 7.2 16 16s-7.2 16-16 16zm0 384c-44.2 0-80-35.8-80-80V416H272v16c0 44.2-35.8 80-80 80z"})})},5457:function(e,n,s){"use strict";s.d(n,{Z:function(){return d}});var i=s(5893),t=s(7294),c=s(2489),r=s(9417),a=s(5869),l=s(9427);function o(e){let{active:n,link:s,alternativeLink:t}=e,c={};return(n==s&&(c.textDecoration="underline"),t)?(0,i.jsx)("a",{style:c,href:"/".concat(t),target:"_blank",rel:"noopener noreferrer",children:s}):(0,i.jsx)("a",{style:c,href:"/".concat(s),children:s})}var h=e=>{let{active:n}=e,{darkmode:s,setDarkmode:h}=(0,t.useContext)(a.UserContext);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)("header",{children:[(0,i.jsx)(o,{active:n,link:"about"}),(0,i.jsx)(o,{active:n,link:"blog"}),(0,i.jsx)(o,{active:n,link:"read"}),(0,i.jsx)(o,{active:n,link:"run"}),(0,i.jsx)(o,{active:n,link:"lang"}),(0,i.jsx)(o,{active:n,link:"cpu"}),(0,i.jsx)(o,{active:n,link:"gpu"}),(0,i.jsx)(o,{active:n,link:"opt"}),(0,i.jsx)(o,{active:n,link:"sw"}),(0,i.jsx)(o,{active:n,link:"ml"}),(0,i.jsx)(o,{active:n,link:"os",alternativeLink:"guidos/book/index.html"})]}),(0,i.jsxs)("div",{className:"box helper-icons",children:[(0,i.jsx)("a",{href:"/feed.xml",children:(0,i.jsx)(c.G,{icon:r.Fwd})}),(0,i.jsx)("a",{onClick:()=>{h(!s)},children:s?l._4:l.R2})]})]})},d=e=>{let{active:n,children:s}=e;return(0,t.useEffect)(()=>{let e=document.querySelectorAll("h1, h2, h3"),n=document.getElementsByClassName("toc");if(0==n.length)return;let s=n[0],i=s.querySelectorAll("a");for(let n of(i.length>0&&i[0].classList.add("active"),window.addEventListener("scroll",n=>{if(void 0!==e&&null!=e&&void 0!==i&&null!=i){let n=window.scrollY;i.forEach((e,n)=>{e.classList.remove("active")});for(var s=e.length-1;s>=0;s--){let t=e[s];if(null!=t&&n>t.offsetTop-75){i[s].classList.add("active");break}}}}),window.addEventListener("click",e=>{s.classList.remove("open")}),e))n.addEventListener("click",e=>{e.stopPropagation(),s.classList.add("open")})},[]),(0,i.jsxs)("div",{children:[(0,i.jsx)(h,{active:n}),(0,i.jsx)("main",{children:s})]})}},6902:function(e,n,s){"use strict";var i=s(5893);n.Z=e=>{let{id:n,description:s,children:t,source:c}=e,r=c?(0,i.jsxs)(i.Fragment,{children:["(See"," ",(0,i.jsx)("a",{href:c,target:"_blank",rel:"noopener noreferrer",children:"source"}),")"]}):(0,i.jsx)(i.Fragment,{});return(0,i.jsxs)("div",{className:"figure",children:[(0,i.jsx)("label",{id:n}),t,(0,i.jsxs)("label",{children:[s," ",r]})]})}},3625:function(e,n,s){"use strict";s.r(n),s.d(n,{default:function(){return o}});var i=s(5893),t=s(1151),c=s(5457),r=s(6902);let a=function(e){let{children:n}=e;return(0,i.jsx)(c.Z,{active:"gpu",children:(0,i.jsx)("div",{className:"content",children:n})})};function l(e){let n={a:"a",blockquote:"blockquote",br:"br",code:"code",div:"div",em:"em",h1:"h1",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",nav:"nav",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.nav,{className:"toc",children:(0,i.jsxs)(n.ol,{className:"toc-level toc-level-1",children:[(0,i.jsx)(n.li,{className:"toc-item toc-item-h1",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h1",href:"#taxonomy",children:"Taxonomy"})}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h1",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h1",href:"#gpgpu",children:"GPGPU"})}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h1",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h1",href:"#opencl",children:"OpenCL"})}),(0,i.jsxs)(n.li,{className:"toc-item toc-item-h1",children:[(0,i.jsx)(n.a,{className:"toc-link toc-link-h1",href:"#cuda-nvidia",children:"CUDA (NVIDIA)"}),(0,i.jsxs)(n.ol,{className:"toc-level toc-level-2",children:[(0,i.jsx)(n.li,{className:"toc-item toc-item-h2",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#terminology",children:"Terminology"})}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h2",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#streaming-multiprocessor-sm",children:"Streaming Multiprocessor (SM)"})}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h2",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#brief",children:"Brief"})}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h2",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#cuda-programming-model",children:"CUDA programming model"})}),(0,i.jsxs)(n.li,{className:"toc-item toc-item-h2",children:[(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#core",children:"Core"}),(0,i.jsxs)(n.ol,{className:"toc-level toc-level-3",children:[(0,i.jsx)(n.li,{className:"toc-item toc-item-h3",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h3",href:"#cuda-core",children:"CUDA core"})}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h3",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h3",href:"#tensor-core",children:"Tensor core"})})]})]}),(0,i.jsxs)(n.li,{className:"toc-item toc-item-h2",children:[(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#units",children:"Units"}),(0,i.jsxs)(n.ol,{className:"toc-level toc-level-3",children:[(0,i.jsx)(n.li,{className:"toc-item toc-item-h3",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h3",href:"#special-function-unit",children:"Special function unit"})}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h3",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h3",href:"#loadstore-unit",children:"Load/store unit"})})]})]}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h2",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#warp",children:"Warp"})}),(0,i.jsxs)(n.li,{className:"toc-item toc-item-h2",children:[(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#memory",children:"Memory"}),(0,i.jsx)(n.ol,{className:"toc-level toc-level-3",children:(0,i.jsx)(n.li,{className:"toc-item toc-item-h3",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h3",href:"#memory-coalescing",children:"Memory coalescing"})})})]}),(0,i.jsxs)(n.li,{className:"toc-item toc-item-h2",children:[(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#drivers-archlinux",children:"Drivers (archlinux)"}),(0,i.jsxs)(n.ol,{className:"toc-level toc-level-3",children:[(0,i.jsx)(n.li,{className:"toc-item toc-item-h3",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h3",href:"#interacting-with-cuda",children:"Interacting with CUDA"})}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h3",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h3",href:"#numba",children:"Numba"})})]})]})]})]}),(0,i.jsxs)(n.li,{className:"toc-item toc-item-h1",children:[(0,i.jsx)(n.a,{className:"toc-link toc-link-h1",href:"#rocm-amd",children:"ROCm (AMD)"}),(0,i.jsx)(n.ol,{className:"toc-level toc-level-2",children:(0,i.jsx)(n.li,{className:"toc-item toc-item-h2",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#hip",children:"HIP"})})})]}),(0,i.jsxs)(n.li,{className:"toc-item toc-item-h1",children:[(0,i.jsx)(n.a,{className:"toc-link toc-link-h1",href:"#software-stacks",children:"Software stacks"}),(0,i.jsxs)(n.ol,{className:"toc-level toc-level-2",children:[(0,i.jsx)(n.li,{className:"toc-item toc-item-h2",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#tinygrad",children:"Tinygrad"})}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h2",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#pytorch",children:"Pytorch"})}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h2",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#jax",children:"Jax"})})]})]}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h1",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h1",href:"#tensor-abstraction",children:"Tensor abstraction"})}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h1",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h1",href:"#define",children:"Define"})}),(0,i.jsxs)(n.li,{className:"toc-item toc-item-h1",children:[(0,i.jsx)(n.a,{className:"toc-link toc-link-h1",href:"#resources",children:"Resources"}),(0,i.jsxs)(n.ol,{className:"toc-level toc-level-2",children:[(0,i.jsx)(n.li,{className:"toc-item toc-item-h2",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#top",children:"Top"})}),(0,i.jsx)(n.li,{className:"toc-item toc-item-h2",children:(0,i.jsx)(n.a,{className:"toc-link toc-link-h2",href:"#other",children:"Other"})})]})]})]})}),"\n",(0,i.jsx)(n.h1,{id:"taxonomy",children:"Taxonomy"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://yosefk.com/blog/simd-simt-smt-parallelism-in-nvidia-gpus.html",children:"https://yosefk.com/blog/simd-simt-smt-parallelism-in-nvidia-gpus.html"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://www.stuffedcow.net/files/gpuarch-ispass2010.pdf",children:"https://www.stuffedcow.net/files/gpuarch-ispass2010.pdf"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"SIMT"}),"\n",(0,i.jsx)(r.Z,{id:"fig-cpu-vs-gpu",description:"Figure 1. CPU vs GPU",source:"https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor",children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"images/cpu-vs-gpu.svg",alt:"alt text",title:"cpu-vs-gpu"})})}),"\n",(0,i.jsx)(n.h1,{id:"gpgpu",children:"GPGPU"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["archlinux wiki for GPGPU: ",(0,i.jsx)(n.a,{href:"https://wiki.archlinux.org/title/GPGPU",children:"https://wiki.archlinux.org/title/GPGPU"})]}),"\n",(0,i.jsxs)(n.li,{children:["what is GPGPU: ",(0,i.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units",children:"https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units"})]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"GPGPU"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"OpenCL"}),"\n",(0,i.jsx)(n.li,{children:"CUDA (NVIDIA)"}),"\n",(0,i.jsx)(n.li,{children:"ROCm (AMD)"}),"\n"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Feature"}),(0,i.jsx)(n.th,{children:"CUDA (NVIDIA)"}),(0,i.jsx)(n.th,{children:"OpenCL (on NVIDIA)"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Support"}),(0,i.jsx)(n.td,{children:"NVIDIA GPUs only"}),(0,i.jsx)(n.td,{children:"NVIDIA, AMD, Intel, CPUs, FPGAs, etc"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Performance"}),(0,i.jsx)(n.td,{children:"Generally higher, more optimized"}),(0,i.jsx)(n.td,{children:"Good, but usually lower than CUDA"})]})]})]}),"\n",(0,i.jsx)(n.h1,{id:"opencl",children:"OpenCL"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["OpenCL: ",(0,i.jsx)(n.a,{href:"https://www.khronos.org/opencl/",children:"https://www.khronos.org/opencl/"})]}),"\n",(0,i.jsxs)(n.li,{children:["Wikipedia: ",(0,i.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/OpenCL",children:"https://en.wikipedia.org/wiki/OpenCL"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["OpenCL is a standard for cross-platform parallel programming of diverse accelerators. As such it is not CUDA specific, but targets CPUs, GPUs, DSPs and FPGAs. The OpenCL API specification enables each chip to have its own OpenCL drivers tuned to its specific architecture. To be conformant to OpenCL, hardware vendors must become OpenCL ",(0,i.jsx)(n.em,{children:"Adapters"})," and submit their conformance test results for review. NVIDIA is OpenCL conformant, which allow us to use the C API provided by OpenCL to directly call the ",(0,i.jsx)(n.em,{children:"NVIDIA runtime"}),". This means that learning OpenCL will translate to other platforms as well, since they have to conform to the OpenCL specification if they are conformant."]}),"\n",(0,i.jsx)(n.h1,{id:"cuda-nvidia",children:"CUDA (NVIDIA)"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"Compute Unified Device Architecture (CUDA)"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"SM -> Warp -> Block -> Grid -> Kernel"}),"\n",(0,i.jsx)(n.h2,{id:"terminology",children:"Terminology"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Streaming Multiprocessors (SM)"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Warp"}),(0,i.jsx)(n.td,{children:"A group of 32 threads."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Warp scheduler"}),(0,i.jsx)(n.td,{children:"A dedicated scheduler for optimizing the execution flow of warps."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Thread group"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"CUDA core"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Tensor core"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Special function unit"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Load/store unit"}),(0,i.jsx)(n.td,{})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"streaming-multiprocessor-sm",children:"Streaming Multiprocessor (SM)"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"Streaming Multiprocessors"})," (SMs) is the fundamental processing unit within NVIDIA GPUs (See ",(0,i.jsx)(n.a,{href:"#fig-h100-sm",children:"Figure 2"})," for an overview of NVIDIA's H100 GPU SM). SMs are roughly analogous to CPU cores. Both execute computations and store state. However, compared to CPU cores, GPU SMs are simple and weaker processors. Execution in SMs is pipelined within an instruction (as in most CPUs since the 1990s), but there is no speculative execution or instruction pointer prediction (as in most CPUs these days)."]}),"\n",(0,i.jsxs)(n.p,{children:["GPU SMs can execute more threads in parallel than CPUs. The H100 GPU can execute more than 16 000 threads (132 SMs x 32 threads x 4 warps > 16 000), while e.g., the ",(0,i.jsx)(n.a,{href:"https://www.techpowerup.com/cpu-specs/epyc-9965.c3904",children:"AMD EPYC 9965"})," uses a total of 384 threads. However, most consumer-grade CPUs are far from even that amount of threads."]}),"\n",(0,i.jsxs)(n.p,{children:["GPU SMs support a large number of ",(0,i.jsx)(n.em,{children:"concurrent"})," threads, i.e., threads that can be interleaved. A single SM on the H100 can concurrently execute up to 2048 threads split across 64 thread groups of 32 threads each. That means that the whole H100 with its 132 SMs can execute a total of 250k concurrent threads."]}),"\n",(0,i.jsx)(r.Z,{id:"fig-h100-sm",description:"Figure 2. A diagram of the internal architecture of an H100 GPU's Streaming Multiprocessors",source:"https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor",children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"images/h100-sm.svg",alt:"alt text",title:"cpu-vs-gpu"})})}),"\n",(0,i.jsx)(n.h2,{id:"brief",children:"Brief"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The host is the CPU available in the system. The system memory associated with the CPU is called host memory."}),"\n",(0,i.jsx)(n.li,{children:"The GPU is called a device and GPU memory likewise called device memory."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"To execute any CUDA program, there are three main steps:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Copy the input data from host memory to device memory, also known as host-to-device transfer."}),"\n",(0,i.jsx)(n.li,{children:"Load the GPU program and execute, caching data on-chip for performance."}),"\n",(0,i.jsx)(n.li,{children:"Copy the results from device memory to host memory, also called device-to-host transfer."}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Every CUDA kernel starts with a ",(0,i.jsx)(n.strong,{children:"global"})," declaration specifier. Programmers provide a unique global ID to each thread by using built-in variables."]}),"\n",(0,i.jsx)(n.p,{children:"A group of threads is called a CUDA block. CUDA blocks are grouped into a grid. A kernel is executed as a grid of blocks of threads (Figure 2)."}),"\n",(0,i.jsx)(n.p,{children:"Each CUDA block is executed by one streaming multiprocessor (SM) and cannot be migrated to other SMs in GPU (except during preemption, debugging, or CUDA dynamic parallelism). One SM can run several concurrent CUDA blocks depending on the resources needed by CUDA blocks. Each kernel is executed on one device and CUDA supports running multiple kernels on a device at one time."}),"\n",(0,i.jsx)(n.p,{children:"The CUDA programming model provides three key language extensions to programmers:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"CUDA blocks—A collection or group of threads."}),"\n",(0,i.jsx)(n.li,{children:"Shared memory—Memory shared within a block among all threads."}),"\n",(0,i.jsx)(n.li,{children:"Synchronization barriers— Enable multiple threads to wait until all threads have reached a particular point of execution before any thread continues."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"There are also many third-party tool-chains available:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/pycuda",children:"PyCUDA"})," — Use CUDA API operations from a Python interface."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/opencl",children:"OpenCL"})," —Use low-level API operations to program CUDA GPUs."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Three step processing in heterogeneous architecture:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"CPU - Bridge - CPU memory"}),"\n",(0,i.jsx)(n.li,{children:"PCIe or NVLink Bus"}),"\n",(0,i.jsx)(n.li,{children:"GigaThread - Interconnect - L2 - DRAM"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"We have to copy from the CPU to the GPU."}),"\n",(0,i.jsx)(n.li,{children:"Do the work, running CUDA kernels."}),"\n",(0,i.jsx)(n.li,{children:"Copy results back to CPU memory from GPU memory."}),"\n"]}),"\n",(0,i.jsx)(n.div,{className:"remark-highlight",children:(0,i.jsx)(n.pre,{className:"language-c",children:(0,i.jsxs)(n.code,{className:"language-c",children:["__global__ ",(0,i.jsx)(n.span,{className:"token keyword",children:"void"})," ",(0,i.jsx)(n.span,{className:"token function",children:"mykernel"}),(0,i.jsx)(n.span,{className:"token punctuation",children:"("}),(0,i.jsx)(n.span,{className:"token keyword",children:"void"}),(0,i.jsx)(n.span,{className:"token punctuation",children:")"})," ",(0,i.jsx)(n.span,{className:"token punctuation",children:"{"}),"\n\n",(0,i.jsx)(n.span,{className:"token punctuation",children:"}"}),"\n"]})})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"__global__"})," indicates that a function:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Runs on the device"}),"\n",(0,i.jsx)(n.li,{children:"Is called from the host code (can also be called from other device code)"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"nvcc"})," separates source code into host and device components:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Device functions (e.g., ",(0,i.jsx)(n.code,{children:"mykernel()"}),") processed by NVIDIA compiler"]}),"\n",(0,i.jsxs)(n.li,{children:["Host functions (e.g., ",(0,i.jsx)(n.code,{children:"main()"}),") processed by standard host compiler."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["We need a way to call this on the GPU. ",(0,i.jsx)(n.em,{children:"Kernel launch"})," or ",(0,i.jsx)(n.em,{children:"triple angle brackets"})," will mark a call to device code:"]}),"\n",(0,i.jsx)(n.div,{className:"remark-highlight",children:(0,i.jsx)(n.pre,{className:"language-c",children:(0,i.jsxs)(n.code,{className:"language-c",children:["mykernel",(0,i.jsx)(n.span,{className:"token operator",children:"<<"}),(0,i.jsx)(n.span,{className:"token operator",children:"<"}),(0,i.jsx)(n.span,{className:"token number",children:"1"}),(0,i.jsx)(n.span,{className:"token punctuation",children:","}),(0,i.jsx)(n.span,{className:"token number",children:"1"}),(0,i.jsx)(n.span,{className:"token operator",children:">>"}),(0,i.jsx)(n.span,{className:"token operator",children:">"}),(0,i.jsx)(n.span,{className:"token punctuation",children:"("}),(0,i.jsx)(n.span,{className:"token punctuation",children:")"}),(0,i.jsx)(n.span,{className:"token punctuation",children:";"}),"\n"]})})}),"\n",(0,i.jsx)(n.p,{children:"The parameters inside the brackets are the CUDA kernel execution configuration."}),"\n",(0,i.jsxs)(n.p,{children:["What about memory management? Host and device memory are separate entities. ",(0,i.jsx)(n.em,{children:"Device"})," pointers point to GPU memory."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Typically passed to device code."}),"\n",(0,i.jsxs)(n.li,{children:["Typically ",(0,i.jsx)(n.strong,{children:"not"})," dereferenced in host code."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"Host"})," pointers point to CPU memory:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Typically ",(0,i.jsx)(n.strong,{children:"not"})," passed to device code."]}),"\n",(0,i.jsxs)(n.li,{children:["Typically ",(0,i.jsx)(n.strong,{children:"not"})," dereferenced in device code."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"(Special cases: Pinned pointers, ATS, managed memory)"}),"\n",(0,i.jsx)(n.p,{children:"Simple CUDA API for handling device memory:"}),"\n",(0,i.jsx)(n.div,{className:"remark-highlight",children:(0,i.jsx)(n.pre,{className:"language-c",children:(0,i.jsxs)(n.code,{className:"language-c",children:[(0,i.jsx)(n.span,{className:"token function",children:"cudaMalloc"}),(0,i.jsx)(n.span,{className:"token punctuation",children:"("}),(0,i.jsx)(n.span,{className:"token punctuation",children:")"}),(0,i.jsx)(n.span,{className:"token punctuation",children:","})," ",(0,i.jsx)(n.span,{className:"token function",children:"cudaFree"}),(0,i.jsx)(n.span,{className:"token punctuation",children:"("}),(0,i.jsx)(n.span,{className:"token punctuation",children:")"}),(0,i.jsx)(n.span,{className:"token punctuation",children:","})," ",(0,i.jsx)(n.span,{className:"token function",children:"cudaMemcpy"}),(0,i.jsx)(n.span,{className:"token punctuation",children:"("}),(0,i.jsx)(n.span,{className:"token punctuation",children:")"})," ",(0,i.jsx)(n.span,{className:"token comment",children:"// similar to C equivalents"}),"\n"]})})}),"\n",(0,i.jsx)(n.h2,{id:"cuda-programming-model",children:"CUDA programming model"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:["The CUDA programming model allows the programmer to define C++ functions, called ",(0,i.jsx)(n.em,{children:"kernels"}),", which are executed N times in parallel by N different CUDA threads when they are invoked (depending on configuration). A kernel is defined using the ",(0,i.jsx)(n.code,{children:"__global__"})," declaration specifier like so:"]}),"\n",(0,i.jsx)(n.div,{className:"remark-highlight",children:(0,i.jsx)(n.pre,{className:"language-c",children:(0,i.jsxs)(n.code,{className:"language-c",children:[(0,i.jsx)(n.span,{className:"token comment",children:"// Kernel definition"}),"\n__global__ ",(0,i.jsx)(n.span,{className:"token keyword",children:"void"})," ",(0,i.jsx)(n.span,{className:"token function",children:"VecAdd"}),(0,i.jsx)(n.span,{className:"token punctuation",children:"("}),(0,i.jsx)(n.span,{className:"token keyword",children:"float"}),(0,i.jsx)(n.span,{className:"token operator",children:"*"})," A",(0,i.jsx)(n.span,{className:"token punctuation",children:","})," ",(0,i.jsx)(n.span,{className:"token keyword",children:"float"}),(0,i.jsx)(n.span,{className:"token operator",children:"*"})," B",(0,i.jsx)(n.span,{className:"token punctuation",children:","})," ",(0,i.jsx)(n.span,{className:"token keyword",children:"float"}),(0,i.jsx)(n.span,{className:"token operator",children:"*"})," C",(0,i.jsx)(n.span,{className:"token punctuation",children:")"}),"\n",(0,i.jsx)(n.span,{className:"token punctuation",children:"{"}),"\n    ",(0,i.jsx)(n.span,{className:"token keyword",children:"int"})," i ",(0,i.jsx)(n.span,{className:"token operator",children:"="})," threadIdx",(0,i.jsx)(n.span,{className:"token punctuation",children:"."}),"x",(0,i.jsx)(n.span,{className:"token punctuation",children:";"}),"\n    C",(0,i.jsx)(n.span,{className:"token punctuation",children:"["}),"i",(0,i.jsx)(n.span,{className:"token punctuation",children:"]"})," ",(0,i.jsx)(n.span,{className:"token operator",children:"="})," A",(0,i.jsx)(n.span,{className:"token punctuation",children:"["}),"i",(0,i.jsx)(n.span,{className:"token punctuation",children:"]"})," ",(0,i.jsx)(n.span,{className:"token operator",children:"+"})," B",(0,i.jsx)(n.span,{className:"token punctuation",children:"["}),"i",(0,i.jsx)(n.span,{className:"token punctuation",children:"]"}),(0,i.jsx)(n.span,{className:"token punctuation",children:";"}),"\n",(0,i.jsx)(n.span,{className:"token punctuation",children:"}"}),"\n",(0,i.jsx)(n.span,{className:"token keyword",children:"int"})," ",(0,i.jsx)(n.span,{className:"token function",children:"main"}),(0,i.jsx)(n.span,{className:"token punctuation",children:"("}),(0,i.jsx)(n.span,{className:"token punctuation",children:")"}),"\n",(0,i.jsx)(n.span,{className:"token punctuation",children:"{"}),"\n    ",(0,i.jsx)(n.span,{className:"token punctuation",children:"."}),(0,i.jsx)(n.span,{className:"token punctuation",children:"."}),(0,i.jsx)(n.span,{className:"token punctuation",children:"."}),"\n    ",(0,i.jsx)(n.span,{className:"token comment",children:"// Kernel invocation with N threads"}),"\n    VecAdd",(0,i.jsx)(n.span,{className:"token operator",children:"<<"}),(0,i.jsx)(n.span,{className:"token operator",children:"<"}),(0,i.jsx)(n.span,{className:"token number",children:"1"}),(0,i.jsx)(n.span,{className:"token punctuation",children:","})," N",(0,i.jsx)(n.span,{className:"token operator",children:">>"}),(0,i.jsx)(n.span,{className:"token operator",children:">"}),(0,i.jsx)(n.span,{className:"token punctuation",children:"("}),"A",(0,i.jsx)(n.span,{className:"token punctuation",children:","})," B",(0,i.jsx)(n.span,{className:"token punctuation",children:","})," C",(0,i.jsx)(n.span,{className:"token punctuation",children:")"}),(0,i.jsx)(n.span,{className:"token punctuation",children:";"}),"\n    ",(0,i.jsx)(n.span,{className:"token punctuation",children:"."}),(0,i.jsx)(n.span,{className:"token punctuation",children:"."}),(0,i.jsx)(n.span,{className:"token punctuation",children:"."}),"\n",(0,i.jsx)(n.span,{className:"token punctuation",children:"}"}),"\n"]})})}),"\n",(0,i.jsxs)(n.p,{children:["Using the ",(0,i.jsx)(n.code,{children:"<<<...>>>"})," syntax we can specify the ",(0,i.jsx)(n.em,{children:"execution configuration"}),". In the above example we specify that the kernel ",(0,i.jsx)(n.code,{children:"VecAdd"})," should be executed on N different CUDA threads."]}),"\n",(0,i.jsx)(n.h2,{id:"core",children:"Core"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"cuda-core",children:"CUDA core"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"tensor-core",children:"Tensor core"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"units",children:"Units"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"special-function-unit",children:"Special function unit"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"loadstore-unit",children:"Load/store unit"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"warp",children:"Warp"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"memory",children:"Memory"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"memory-coalescing",children:"Memory coalescing"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["memory coalescing: ",(0,i.jsx)(n.a,{href:"https://homepages.math.uic.edu/~jan/mcs572f16/mcs572notes/lec35.html",children:"https://homepages.math.uic.edu/~jan/mcs572f16/mcs572notes/lec35.html"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"drivers-archlinux",children:"Drivers (archlinux)"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:"Run"}),"\n",(0,i.jsx)(n.div,{className:"remark-highlight",children:(0,i.jsx)(n.pre,{className:"language-sh",children:(0,i.jsxs)(n.code,{className:"language-sh",children:["$ lspci ",(0,i.jsx)(n.span,{className:"token parameter variable",children:"-k"})," ",(0,i.jsx)(n.span,{className:"token parameter variable",children:"-d"})," ::03xx\n01:00.0 VGA compatible controller: NVIDIA Corporation GP104 ",(0,i.jsx)(n.span,{className:"token punctuation",children:"["}),"GeForce GTX ",(0,i.jsx)(n.span,{className:"token number",children:"1070"}),(0,i.jsx)(n.span,{className:"token punctuation",children:"]"})," ",(0,i.jsx)(n.span,{className:"token punctuation",children:"("}),"rev a1",(0,i.jsx)(n.span,{className:"token punctuation",children:")"}),"\n	Subsystem: ASUSTeK Computer Inc. Device ",(0,i.jsx)(n.span,{className:"token number",children:"8599"}),"\n	Kernel driver ",(0,i.jsx)(n.span,{className:"token keyword",children:"in"})," use: nouveau\n	Kernel modules: nouveau, nvidia_drm, nvidia\n"]})})}),"\n",(0,i.jsx)(n.p,{children:"Look for your name:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://nouveau.freedesktop.org/CodeNames.html",children:"https://nouveau.freedesktop.org/CodeNames.html"})}),"\n",(0,i.jsxs)(n.p,{children:["Install correct package according to:\n",(0,i.jsxs)(n.a,{href:"https://wiki.archlinux.org/title/NVIDIA#Installation",children:["https://wiki.archlinux.org/title/NVIDIA","https://wiki.archlinux.org/title/NVIDIA#Installation"]})]}),"\n",(0,i.jsx)(n.p,{children:"Restart machine"}),"\n",(0,i.jsx)(n.div,{className:"remark-highlight",children:(0,i.jsx)(n.pre,{className:"language-unknown",children:(0,i.jsx)(n.code,{className:"language-unknown",children:"nvidia-smi"})})}),"\n",(0,i.jsxs)(n.p,{children:["Install cuda (also see ",(0,i.jsx)(n.a,{href:"https://docs.nvidia.com/deploy/cuda-compatibility/",children:"https://docs.nvidia.com/deploy/cuda-compatibility/"}),")"]}),"\n",(0,i.jsx)(n.div,{className:"remark-highlight",children:(0,i.jsx)(n.pre,{className:"language-sh",children:(0,i.jsxs)(n.code,{className:"language-sh",children:["pacman ",(0,i.jsx)(n.span,{className:"token parameter variable",children:"-S"})," cuda\n"]})})}),"\n",(0,i.jsx)(n.p,{children:"Verify installation"}),"\n",(0,i.jsx)(n.div,{className:"remark-highlight",children:(0,i.jsx)(n.pre,{className:"language-unknown",children:(0,i.jsx)(n.code,{className:"language-unknown",children:"nvcc --version\n"})})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"nvidia"}),"\n",(0,i.jsx)(n.li,{children:"nvidia-utils: provides nvidia-libgl, opengl-driver, vulkan-driver"}),"\n",(0,i.jsx)(n.li,{children:"cuda"}),"\n",(0,i.jsx)(n.li,{children:"opencl-nvidia"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"interacting-with-cuda",children:"Interacting with CUDA"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:"There exists several wrappers of the CUDA API."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["PyCUDA: ",(0,i.jsx)(n.a,{href:"https://github.com/inducer/pycuda",children:"https://github.com/inducer/pycuda"})," ",(0,i.jsx)(n.br,{}),"\n","Lets you access NVIDIA's CUDA parallel computation API from Python."]}),"\n",(0,i.jsxs)(n.li,{children:["Official CUDA API bindings: ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA/cuda-python",children:"https://github.com/NVIDIA/cuda-python"})," ",(0,i.jsx)(n.br,{}),"\n","NVIDIA's own language bindings for the CUDA API written in Python/Cython. This is similar to (1) in abstraction level."]}),"\n",(0,i.jsxs)(n.li,{children:["Numba: ",(0,i.jsx)(n.a,{href:"https://github.com/numba/numba",children:"https://github.com/numba/numba"})," ",(0,i.jsx)(n.br,{}),"\n","NumPy-aware optimizing compiler for Python, which means it uses the LLVM compiler project to generate machine code directly from Python syntax. Numba provides a high-level interface and uses cuda-python (2) under the hood to accelerate computations when compiling for CUDA."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"numba",children:"Numba"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"Numba is an open source, NumPy-aware optimizing compiler for Python sponsored by Anaconda, Inc. It uses the LLVM compiler project to generate machine code from Python syntax.\nNumba can compile a large subset of numerically-focused Python, including many NumPy functions. Additionally, Numba has support for automatic parallelization of loops, generation of GPU-accelerated code, and creation of ufuncs and C callbacks."}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Numba: ",(0,i.jsx)(n.a,{href:"https://numba.pydata.org/",children:"https://numba.pydata.org/"})]}),"\n",(0,i.jsxs)(n.li,{children:["Numba github: ",(0,i.jsx)(n.a,{href:"https://github.com/numba/numba",children:"https://github.com/numba/numba"})]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://numba.readthedocs.io/en/stable/cuda/index.html",children:"https://numba.readthedocs.io/en/stable/cuda/index.html"})}),"\n"]}),"\n",(0,i.jsx)(n.h1,{id:"rocm-amd",children:"ROCm (AMD)"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"ROCm"})," (Radeon Open Compute) is AMD's open-source parallel computing architecture and framework."]}),"\n",(0,i.jsx)(n.h2,{id:"hip",children:"HIP"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"HIP"})," (Heterogeneous Interface for Portability) is AMD's dedicated GPU programming environment."]}),"\n",(0,i.jsx)(n.h1,{id:"software-stacks",children:"Software stacks"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Tinygrad"}),"\n",(0,i.jsx)(n.li,{children:"Pytorch"}),"\n",(0,i.jsx)(n.li,{children:"Jax"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"tinygrad",children:"Tinygrad"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:"Tinygrad takes advantage of the fact that every tensor operation is either elementwise, or reduction. The obvious advantage is to build up abstraction that makes optimization easier (think of this in the context of Complex Instruction Set Architecture vs RISC)."}),"\n",(0,i.jsx)(n.h2,{id:"pytorch",children:"Pytorch"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"jax",children:"Jax"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://github.com/jax-ml/jax",children:"https://github.com/jax-ml/jax"})}),"\n",(0,i.jsx)(n.p,{children:"JAX is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning."}),"\n",(0,i.jsx)(n.p,{children:"JAX can automatically differentiate native Python and NumPy functions. It can differentiate through loops, branches, recursion, and closures, and it can take derivatives of derivatives of derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation) via jax.grad as well as forward-mode differentiation, and the two can be composed arbitrarily to any order."}),"\n",(0,i.jsx)(n.p,{children:"JAX uses XLA to compile and scale your NumPy programs on TPUs, GPUs, and other hardware accelerators."}),"\n",(0,i.jsx)(n.h1,{id:"tensor-abstraction",children:"Tensor abstraction"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:["A ",(0,i.jsx)(n.em,{children:"Tensor"})," is a multi-dimensional matrix containing elements of a single data type."]}),"\n",(0,i.jsx)(n.h1,{id:"define",children:"Define"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"SM"}),"\n",(0,i.jsx)(n.li,{children:"CUDA programming model"}),"\n",(0,i.jsx)(n.li,{children:"GPUs are 1024 bit SIMD machines?"}),"\n"]}),"\n",(0,i.jsx)(n.h1,{id:"resources",children:"Resources"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"top",children:"Top"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["cuda c programming: ",(0,i.jsx)(n.a,{href:"https://docs.nvidia.com/cuda/cuda-c-programming-guide/",children:"https://docs.nvidia.com/cuda/cuda-c-programming-guide/"})]}),"\n",(0,i.jsxs)(n.li,{children:["Really nice glossary: ",(0,i.jsx)(n.a,{href:"https://modal.com/gpu-glossary",children:"https://modal.com/gpu-glossary"})]}),"\n",(0,i.jsxs)(n.li,{children:["cuda-training-series: ",(0,i.jsx)(n.a,{href:"https://www.olcf.ornl.gov/cuda-training-series/",children:"https://www.olcf.ornl.gov/cuda-training-series/"})]}),"\n",(0,i.jsxs)(n.li,{children:["cuda best practices: ",(0,i.jsxs)(n.a,{href:"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#who-should-read-this-guide",children:["https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html","https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#who-should-read-this-guide"]})]}),"\n",(0,i.jsxs)(n.li,{children:["going further than cuda intro (",(0,i.jsx)(n.em,{children:"exercises"})," and ",(0,i.jsx)(n.em,{children:"where to from here"}),"): ",(0,i.jsxs)(n.a,{href:"https://developer.nvidia.com/blog/even-easier-introduction-cuda/#exercises",children:["https://developer.nvidia.com/blog/even-easier-introduction-cuda/","https://developer.nvidia.com/blog/even-easier-introduction-cuda/#exercises"]})]}),"\n",(0,i.jsxs)(n.li,{children:["demystify gpu for dl part1 (blog): ",(0,i.jsx)(n.a,{href:"https://learnopencv.com/demystifying-gpu-architectures-for-deep-learning/",children:"https://learnopencv.com/demystifying-gpu-architectures-for-deep-learning/"})]}),"\n",(0,i.jsxs)(n.li,{children:["demystify gpu for dl part2 (blog): ",(0,i.jsx)(n.a,{href:"https://learnopencv.com/demystifying-gpu-architectures-for-deep-learning-part-2/",children:"https://learnopencv.com/demystifying-gpu-architectures-for-deep-learning-part-2/"})]}),"\n",(0,i.jsxs)(n.li,{children:["modern gpu architecture explained (blog): ",(0,i.jsx)(n.a,{href:"https://learnopencv.com/modern-gpu-architecture-explained/",children:"https://learnopencv.com/modern-gpu-architecture-explained/"})]}),"\n",(0,i.jsxs)(n.li,{children:["leetgpu: ",(0,i.jsx)(n.a,{href:"https://leetgpu.com/",children:"https://leetgpu.com/"})]}),"\n",(0,i.jsxs)(n.li,{children:["leetgpu resources: ",(0,i.jsx)(n.a,{href:"https://www.leetgpu.com/resources",children:"https://www.leetgpu.com/resources"})]}),"\n",(0,i.jsxs)(n.li,{children:["supercomputing: ",(0,i.jsx)(n.a,{href:"https://homepages.math.uic.edu/~jan/mcs572f16/mcs572notes/index.html",children:"https://homepages.math.uic.edu/~jan/mcs572f16/mcs572notes/index.html"})]}),"\n",(0,i.jsxs)(n.li,{children:["fabiensanglard (blog): ",(0,i.jsx)(n.a,{href:"https://fabiensanglard.net/cuda/",children:"https://fabiensanglard.net/cuda/"})]}),"\n",(0,i.jsxs)(n.li,{children:["paul richmond lecture series: ",(0,i.jsx)(n.a,{href:"https://paulrichmond.shef.ac.uk//education/com4521",children:"https://paulrichmond.shef.ac.uk//education/com4521"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"other",children:"Other"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["CUDA programming guide: ",(0,i.jsx)(n.a,{href:"https://docs.nvidia.com/cuda/cuda-c-programming-guide/",children:"https://docs.nvidia.com/cuda/cuda-c-programming-guide/"})]}),"\n",(0,i.jsxs)(n.li,{children:["CUDA books archive: ",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/cuda-books-archive",children:"https://developer.nvidia.com/cuda-books-archive"})]}),"\n",(0,i.jsxs)(n.li,{children:["CUDA parallel programming: ",(0,i.jsx)(n.a,{href:"https://newfrontiers.illinois.edu/news-and-events/introduction-to-parallel-programming-with-cuda/",children:"https://newfrontiers.illinois.edu/news-and-events/introduction-to-parallel-programming-with-cuda/"})]}),"\n",(0,i.jsxs)(n.li,{children:["Triton: ",(0,i.jsx)(n.a,{href:"https://triton-lang.org/main/getting-started/tutorials/index.html",children:"https://triton-lang.org/main/getting-started/tutorials/index.html"})]}),"\n",(0,i.jsxs)(n.li,{children:["Pytorch doc: ",(0,i.jsx)(n.a,{href:"https://docs.pytorch.org/docs/stable/index.html",children:"https://docs.pytorch.org/docs/stable/index.html"})]}),"\n",(0,i.jsxs)(n.li,{children:["Pytorch tutorials: ",(0,i.jsx)(n.a,{href:"https://docs.pytorch.org/tutorials/",children:"https://docs.pytorch.org/tutorials/"})]}),"\n",(0,i.jsxs)(n.li,{children:["Pytorch zero to mastery: ",(0,i.jsx)(n.a,{href:"https://www.learnpytorch.io/",children:"https://www.learnpytorch.io/"})]}),"\n",(0,i.jsxs)(n.li,{children:["Tinygrad doc: ",(0,i.jsx)(n.a,{href:"https://docs.tinygrad.org/",children:"https://docs.tinygrad.org/"})]}),"\n",(0,i.jsxs)(n.li,{children:["Tinygrad notes: ",(0,i.jsx)(n.a,{href:"https://mesozoic-egg.github.io/tinygrad-notes/",children:"https://mesozoic-egg.github.io/tinygrad-notes/"})]}),"\n",(0,i.jsxs)(n.li,{children:["Mojo: ",(0,i.jsx)(n.a,{href:"https://docs.modular.com/mojo/manual/",children:"https://docs.modular.com/mojo/manual/"})]}),"\n",(0,i.jsxs)(n.li,{children:["cuda refresher: ",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/blog/tag/cuda-refresher/",children:"https://developer.nvidia.com/blog/tag/cuda-refresher/"})]}),"\n",(0,i.jsxs)(n.li,{children:["cuda refresher programming model: ",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/",children:"https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/"})]}),"\n",(0,i.jsxs)(n.li,{children:["cuda c programming pdf: ",(0,i.jsx)(n.a,{href:"https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf",children:"https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf"})]}),"\n",(0,i.jsxs)(n.li,{children:["nvidia tesla paper: ",(0,i.jsx)(n.a,{href:"https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf",children:"https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf"})]}),"\n",(0,i.jsxs)(n.li,{children:["cuda in python part1: ",(0,i.jsx)(n.a,{href:"https://www.vincent-lunot.com/post/an-introduction-to-cuda-in-python-part-1/",children:"https://www.vincent-lunot.com/post/an-introduction-to-cuda-in-python-part-1/"})]}),"\n",(0,i.jsxs)(n.li,{children:["cuda compatibility: ",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/cuda-gpus",children:"https://developer.nvidia.com/cuda-gpus"})]}),"\n",(0,i.jsxs)(n.li,{children:["nvidia hpc: ",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/hpc",children:"https://developer.nvidia.com/hpc"})]}),"\n",(0,i.jsxs)(n.li,{children:["archlinux gpgpu: ",(0,i.jsx)(n.a,{href:"https://wiki.archlinux.org/title/GPGPU",children:"https://wiki.archlinux.org/title/GPGPU"})]}),"\n",(0,i.jsxs)(n.li,{children:["cuda python: ",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/cuda-python",children:"https://developer.nvidia.com/cuda-python"})]}),"\n",(0,i.jsxs)(n.li,{children:["cuda python doc: ",(0,i.jsx)(n.a,{href:"https://nvidia.github.io/cuda-python/latest/",children:"https://nvidia.github.io/cuda-python/latest/"})]}),"\n",(0,i.jsxs)(n.li,{children:["cuda python github: ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA/cuda-python/tree/main",children:"https://github.com/NVIDIA/cuda-python/tree/main"})]}),"\n",(0,i.jsxs)(n.li,{children:["opencl: ",(0,i.jsxs)(n.a,{href:"https://www.khronos.org/opencl/#ocl-overview",children:["https://www.khronos.org/opencl/","https://www.khronos.org/opencl/#ocl-overview"]})]}),"\n",(0,i.jsxs)(n.li,{children:["archlinux nvidia: ",(0,i.jsx)(n.a,{href:"https://wiki.archlinux.org/title/NVIDIA",children:"https://wiki.archlinux.org/title/NVIDIA"})]}),"\n",(0,i.jsxs)(n.li,{children:["nvidia codenames: ",(0,i.jsx)(n.a,{href:"https://nouveau.freedesktop.org/CodeNames.html",children:"https://nouveau.freedesktop.org/CodeNames.html"})]}),"\n",(0,i.jsxs)(n.li,{children:["numba nvidia guide: ",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/blog/numba-python-cuda-acceleration/",children:"https://developer.nvidia.com/blog/numba-python-cuda-acceleration/"})]}),"\n",(0,i.jsxs)(n.li,{children:["numba docs: ",(0,i.jsx)(n.a,{href:"https://numba.readthedocs.io/en/stable/",children:"https://numba.readthedocs.io/en/stable/"})]}),"\n",(0,i.jsxs)(n.li,{children:["nvidia tech blog: ",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/blog/",children:"https://developer.nvidia.com/blog/"})]}),"\n",(0,i.jsxs)(n.li,{children:["tiny-gpu: ",(0,i.jsx)(n.a,{href:"https://github.com/adam-maj/tiny-gpu",children:"https://github.com/adam-maj/tiny-gpu"})]}),"\n",(0,i.jsxs)(n.li,{children:["cuda-training-series github: ",(0,i.jsx)(n.a,{href:"https://github.com/olcf/cuda-training-series/tree/master/exercises",children:"https://github.com/olcf/cuda-training-series/tree/master/exercises"})]}),"\n",(0,i.jsxs)(n.li,{children:["easy intro to cuda: ",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/blog/even-easier-introduction-cuda/",children:"https://developer.nvidia.com/blog/even-easier-introduction-cuda/"})]}),"\n",(0,i.jsxs)(n.li,{children:["cub: ",(0,i.jsx)(n.a,{href:"https://nvidia.github.io/cccl/cub/",children:"https://nvidia.github.io/cccl/cub/"})]}),"\n",(0,i.jsxs)(n.li,{children:["cutlass: cuda templates for linear algebra subroutines: ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA/cutlass",children:"https://github.com/NVIDIA/cutlass"})]}),"\n",(0,i.jsxs)(n.li,{children:["gpu-puzzles: ",(0,i.jsx)(n.a,{href:"https://github.com/srush/GPU-Puzzles",children:"https://github.com/srush/GPU-Puzzles"})]}),"\n",(0,i.jsxs)(n.li,{children:["what is a gpu: ",(0,i.jsx)(n.a,{href:"https://kernel-operations.io/keops/autodiff_gpus/what_is_a_gpu.html",children:"https://kernel-operations.io/keops/autodiff_gpus/what_is_a_gpu.html"})]}),"\n",(0,i.jsxs)(n.li,{children:["experiment with printf cuda: ",(0,i.jsxs)(n.a,{href:"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#formatted-output",children:["https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html","https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#formatted-output"]})]}),"\n",(0,i.jsxs)(n.li,{children:["applied GPU programming: ",(0,i.jsx)(n.a,{href:"https://www.youtube.com/playlist?list=PLPJwWVtf19Wgx_bupSDDSStSv-tOGGWRO",children:"https://www.youtube.com/playlist?list=PLPJwWVtf19Wgx_bupSDDSStSv-tOGGWRO"})]}),"\n",(0,i.jsxs)(n.li,{children:["cuda ptx interop: ",(0,i.jsx)(n.a,{href:"https://docs.nvidia.com/cuda/pdf/PTX_Writers_Guide_To_Interoperability.pdf",children:"https://docs.nvidia.com/cuda/pdf/PTX_Writers_Guide_To_Interoperability.pdf"})]}),"\n",(0,i.jsxs)(n.li,{children:["gpu mode: ",(0,i.jsxs)(n.a,{href:"https://www.youtube.com/@GPUMODE",children:["https://www.youtube.com/","(",")"]})]}),"\n",(0,i.jsxs)(n.li,{children:["numba cuda: ",(0,i.jsx)(n.a,{href:"https://nvidia.github.io/numba-cuda/user/index.html",children:"https://nvidia.github.io/numba-cuda/user/index.html"})]}),"\n",(0,i.jsxs)(n.li,{children:["opencl registry: ",(0,i.jsx)(n.a,{href:"https://registry.khronos.org/OpenCL/",children:"https://registry.khronos.org/OpenCL/"})]}),"\n",(0,i.jsxs)(n.li,{children:["turing arch whitepaper: ",(0,i.jsx)(n.a,{href:"https://fabiensanglard.net/cuda/Turing-Architecture-Whitepaper.pdf",children:"https://fabiensanglard.net/cuda/Turing-Architecture-Whitepaper.pdf"})]}),"\n",(0,i.jsxs)(n.li,{children:["rust GPU: ",(0,i.jsx)(n.a,{href:"https://rust-gpu.github.io/blog/2025/07/25/rust-on-every-gpu/",children:"https://rust-gpu.github.io/blog/2025/07/25/rust-on-every-gpu/"})]}),"\n"]})]})}function o(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};return(0,i.jsx)(a,{...e,children:(0,i.jsx)(l,{...e})})}},1151:function(e,n,s){"use strict";s.d(n,{a:function(){return c}});var i=s(7294);let t=i.createContext({});function c(e){let n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}}},function(e){e.O(0,[976,603,774,888,179],function(){return e(e.s=4478)}),_N_E=e.O()}]);