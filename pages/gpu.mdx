import Page from "../components/page";

# Tensor abstraction

A _Tensor_ is a multi-dimensional matrix containing elements of a single data type.

# GPGPU

- archlinux wiki for GPGPU: https://wiki.archlinux.org/title/GPGPU
- what is GPGPU: https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units

## OpenCL

- OpenCL: https://www.khronos.org/opencl/
- Wikipedia: https://en.wikipedia.org/wiki/OpenCL

OpenCL is a standard for cross-platform parallel programming of diverse accelerators. As such it is not CUDA specific, but targets CPUs, GPUs, DSPs and FPGAs. The OpenCL API specification enables each chip to have its own OpenCL drivers tuned to its specific architecture. To be conformant to OpenCL, hardware vendors must become OpenCL _Adapters_ and submit their conformance test results for review. Nvidia is OpenCL conformant, which allow us to use the C API provided by OpenCL to directly call the _Nvidia runtime_. This means that learning OpenCL will translate to other platforms as well, since they have to conform to the OpenCL specification if they are conformant.

| Feature     | CUDA (Nvidia)                    | OpenCL (on Nvidia)                   |
| ----------- | -------------------------------- | ------------------------------------ |
| Support     | Nvidia GPUs only                 | Nvidia, AMD, Intel, CPUs, FPGAs, etc |
| Performance | Generally higher, more optimized | Good, but usually lower than CUDA    |

## CUDA

The host is the CPU available in the system. The system memory associated with the CPU is called host memory. The GPU is called a device and GPU memory likewise called device memory.

To execute any CUDA program, there are three main steps:

- Copy the input data from host memory to device memory, also known as host-to-device transfer.
- Load the GPU program and execute, caching data on-chip for performance.
- Copy the results from device memory to host memory, also called device-to-host transfer.

Every CUDA kernel starts with a **global** declaration specifier. Programmers provide a unique global ID to each thread by using built-in variables.

A group of threads is called a CUDA block. CUDA blocks are grouped into a grid. A kernel is executed as a grid of blocks of threads (Figure 2).

Each CUDA block is executed by one streaming multiprocessor (SM) and cannot be migrated to other SMs in GPU (except during preemption, debugging, or CUDA dynamic parallelism). One SM can run several concurrent CUDA blocks depending on the resources needed by CUDA blocks. Each kernel is executed on one device and CUDA supports running multiple kernels on a device at one time.

The CUDA programming model provides three key language extensions to programmers:

- CUDA blocks—A collection or group of threads.
- Shared memory—Memory shared within a block among all threads.
- Synchronization barriers— Enable multiple threads to wait until all threads have reached a particular point of execution before any thread continues.

There are also many third-party tool-chains available:

- [PyCUDA](https://developer.nvidia.com/pycuda) — Use CUDA API operations from a Python interface.
- [OpenCL](https://developer.nvidia.com/opencl) —Use low-level API operations to program CUDA GPUs.

### Install driver (archlinux)

Run

```sh
$ lspci -k -d ::03xx
01:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)
	Subsystem: ASUSTeK Computer Inc. Device 8599
	Kernel driver in use: nouveau
	Kernel modules: nouveau, nvidia_drm, nvidia
```

Look for your name:

https://nouveau.freedesktop.org/CodeNames.html

Install correct package according to:
https://wiki.archlinux.org/title/NVIDIA#Installation

Restart machine

```
nvidia-smi
```

Install cuda (also see https://docs.nvidia.com/deploy/cuda-compatibility/)

```sh
pacman -S cuda
```

Verify installation

```
nvcc --version

```

- nvidia
- nvidia-utils: provides nvidia-libgl, opengl-driver, vulkan-driver
- cuda
- opencl-nvidia

### Interacting with CUDA

There exists several wrappers of the CUDA API.

1. PyCUDA: https://github.com/inducer/pycuda \
   Lets you access Nvidia's CUDA parallel computation API from Python.
2. Official CUDA API bindings: https://github.com/NVIDIA/cuda-python \
   Nvidia's own language bindings for the CUDA API written in Python/Cython. This is similar to (1) in abstraction level.
3. Numba: https://github.com/numba/numba \
   NumPy-aware optimizing compiler for Python, which means it uses the LLVM compiler project to generate machine code directly from Python syntax. Numba provides a high-level interface and uses cuda-python (2) under the hood to accelerate computations when compiling for CUDA.

### Numba

> Numba is an open source, NumPy-aware optimizing compiler for Python sponsored by Anaconda, Inc. It uses the LLVM compiler project to generate machine code from Python syntax.
> Numba can compile a large subset of numerically-focused Python, including many NumPy functions. Additionally, Numba has support for automatic parallelization of loops, generation of GPU-accelerated code, and creation of ufuncs and C callbacks.

- Numba: https://numba.pydata.org/
- Numba github: https://github.com/numba/numba
- https://numba.readthedocs.io/en/stable/cuda/index.html

## ROCm

_ROCm_ (Radeon Open Compute) is AMD's open-source parallel computing architecture and framework.

### HIP

_HIP_ (Heterogeneous Interface for Portability) is AMD's dedicated GPU programming environment.

## Tinygrad

Tinygrad takes advantage of the fact that every tensor operation is either elementwise, or reduction. The obvious advantage is to build up abstraction that makes optimization easier (think of this in the context of Complex Instruction Set Architecture vs RISC).

## Pytorch

lune

## Jax

https://github.com/jax-ml/jax

JAX is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning.

JAX can automatically differentiate native Python and NumPy functions. It can differentiate through loops, branches, recursion, and closures, and it can take derivatives of derivatives of derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation) via jax.grad as well as forward-mode differentiation, and the two can be composed arbitrarily to any order.

JAX uses XLA to compile and scale your NumPy programs on TPUs, GPUs, and other hardware accelerators.

# Define

- SM
- CUDA programming model

# Resources

- cuda refresher: https://developer.nvidia.com/blog/tag/cuda-refresher/
- cuda refresher programming model: https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/
- cuda c programming: https://docs.nvidia.com/cuda/cuda-c-programming-guide/
- cuda c programming pdf: https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf
- nvidia tesla paper: https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf
- cuda in python part1: https://www.vincent-lunot.com/post/an-introduction-to-cuda-in-python-part-1/
- cuda compatibility: https://developer.nvidia.com/cuda-gpus
- nvidia hpc: https://developer.nvidia.com/hpc
- archlinux gpgpu: https://wiki.archlinux.org/title/GPGPU
- cuda python: https://developer.nvidia.com/cuda-python
- cuda python doc: https://nvidia.github.io/cuda-python/latest/
- cuda python github: https://github.com/NVIDIA/cuda-python/tree/main
- opencl: https://www.khronos.org/opencl/#ocl-overview
- archlinux nvidia: https://wiki.archlinux.org/title/NVIDIA
- nvidia codenames: https://nouveau.freedesktop.org/CodeNames.html
- numba nvidia guide: https://developer.nvidia.com/blog/numba-python-cuda-acceleration/
- numba docs: https://numba.readthedocs.io/en/stable/
- nvidia tech blog: https://developer.nvidia.com/blog/

general

- Leetcpu: https://www.leetgpu.com/resources
- Really nice glossary: https://modal.com/gpu-glossary
- CUDA programming guide: https://docs.nvidia.com/cuda/cuda-c-programming-guide/
- CUDA books archive: https://developer.nvidia.com/cuda-books-archive
- CUDA parallel programming: https://newfrontiers.illinois.edu/news-and-events/introduction-to-parallel-programming-with-cuda/
- Triton: https://triton-lang.org/main/getting-started/tutorials/index.html
- Pytorch doc: https://docs.pytorch.org/docs/stable/index.html
- Pytorch tutorials: https://docs.pytorch.org/tutorials/
- Pytorch zero to mastery: https://www.learnpytorch.io/
- Tinygrad doc: https://docs.tinygrad.org/
- Tinygrad notes: https://mesozoic-egg.github.io/tinygrad-notes/
- Mojo: https://docs.modular.com/mojo/manual/

export default function MDXPage({ children }) {
  return (
    <Page active="gpu">
      <div className="content">{children}</div>
    </Page>
  );
}
