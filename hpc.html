<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/f1431a11044d3e28.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f1431a11044d3e28.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-3d79c14ccb9ae0b4.js" defer=""></script><script src="/_next/static/chunks/framework-305cb810cde7afac.js" defer=""></script><script src="/_next/static/chunks/main-1e819d43c82e61a4.js" defer=""></script><script src="/_next/static/chunks/pages/_app-cc4af0cbd72d930f.js" defer=""></script><script src="/_next/static/chunks/c16184b3-1754bfa260bbde28.js" defer=""></script><script src="/_next/static/chunks/603-9686efb8fc297326.js" defer=""></script><script src="/_next/static/chunks/pages/hpc-ff98262d977f0cc5.js" defer=""></script><script src="/_next/static/aa3v2nN24CrFndNCoWAn5/_buildManifest.js" defer=""></script><script src="/_next/static/aa3v2nN24CrFndNCoWAn5/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div><header><a href="/about">about</a><a href="/blog">blog</a><a href="/read">read</a><a href="/run">run</a><a href="/c">c</a><a href="/guidos/book/index.html" target="_blank" rel="noopener noreferrer">os</a><a href="/ml">ml</a><a style="text-decoration:underline" href="/hpc">hpc</a></header><div class="box helper-icons"><a href="/feed.xml"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="rss" class="svg-inline--fa fa-rss " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 64C0 46.3 14.3 32 32 32c229.8 0 416 186.2 416 416c0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96C14.3 96 0 81.7 0 64zM0 416a64 64 0 1 1 128 0A64 64 0 1 1 0 416zM32 160c159.1 0 288 128.9 288 288c0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224c-17.7 0-32-14.3-32-32s14.3-32 32-32z"></path></svg></a><a><svg style="position:relative;top:2px" xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 384 512"><path d="M297.2 248.9C311.6 228.3 320 203.2 320 176c0-70.7-57.3-128-128-128S64 105.3 64 176c0 27.2 8.4 52.3 22.8 72.9c3.7 5.3 8.1 11.3 12.8 17.7l0 0c12.9 17.7 28.3 38.9 39.8 59.8c10.4 19 15.7 38.8 18.3 57.5H109c-2.2-12-5.9-23.7-11.8-34.5c-9.9-18-22.2-34.9-34.5-51.8l0 0 0 0c-5.2-7.1-10.4-14.2-15.4-21.4C27.6 247.9 16 213.3 16 176C16 78.8 94.8 0 192 0s176 78.8 176 176c0 37.3-11.6 71.9-31.4 100.3c-5 7.2-10.2 14.3-15.4 21.4l0 0 0 0c-12.3 16.8-24.6 33.7-34.5 51.8c-5.9 10.8-9.6 22.5-11.8 34.5H226.4c2.6-18.7 7.9-38.6 18.3-57.5c11.5-20.9 26.9-42.1 39.8-59.8l0 0 0 0 0 0c4.7-6.4 9-12.4 12.7-17.7zM192 128c-26.5 0-48 21.5-48 48c0 8.8-7.2 16-16 16s-16-7.2-16-16c0-44.2 35.8-80 80-80c8.8 0 16 7.2 16 16s-7.2 16-16 16zm0 384c-44.2 0-80-35.8-80-80V416H272v16c0 44.2-35.8 80-80 80z"></path></svg></a></div><main><div class="content"><nav class="toc"><ol class="toc-level toc-level-1"><li class="toc-item toc-item-h1"><a class="toc-link toc-link-h1" href="#gpu">GPU</a><ol class="toc-level toc-level-2"><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#cuda">CUDA</a><ol class="toc-level toc-level-3"><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#install-driver-archlinux">Install driver (archlinux)</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#python-and-cuda">Python and CUDA</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#numba">Numba</a></li></ol></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#define">Define</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#resources">Resources</a></li></ol></li></ol></nav>
<h1 id="gpu">GPU</h1>
<hr/>
<h2 id="cuda">CUDA</h2>
<hr/>
<p>The host is the CPU available in the system. The system memory associated with the CPU is called host memory. The GPU is called a device and GPU memory likewise called device memory.</p>
<p>To execute any CUDA program, there are three main steps:</p>
<ul>
<li>Copy the input data from host memory to device memory, also known as host-to-device transfer.</li>
<li>Load the GPU program and execute, caching data on-chip for performance.</li>
<li>Copy the results from device memory to host memory, also called device-to-host transfer.</li>
</ul>
<p>Every CUDA kernel starts with a <strong>global</strong> declaration specifier. Programmers provide a unique global ID to each thread by using built-in variables.</p>
<p>A group of threads is called a CUDA block. CUDA blocks are grouped into a grid. A kernel is executed as a grid of blocks of threads (Figure 2).</p>
<p>Each CUDA block is executed by one streaming multiprocessor (SM) and cannot be migrated to other SMs in GPU (except during preemption, debugging, or CUDA dynamic parallelism). One SM can run several concurrent CUDA blocks depending on the resources needed by CUDA blocks. Each kernel is executed on one device and CUDA supports running multiple kernels on a device at one time.</p>
<p>The CUDA programming model provides three key language extensions to programmers:</p>
<ul>
<li>CUDA blocks—A collection or group of threads.</li>
<li>Shared memory—Memory shared within a block among all threads.</li>
<li>Synchronization barriers— Enable multiple threads to wait until all threads have reached a particular point of execution before any thread continues.</li>
</ul>
<p>There are also many third-party tool-chains available:</p>
<ul>
<li><a href="https://developer.nvidia.com/pycuda">PyCUDA</a> — Use CUDA API operations from a Python interface.</li>
<li><a href="https://developer.nvidia.com/opencl">OpenCL</a> —Use low-level API operations to program CUDA GPUs.</li>
</ul>
<h3 id="install-driver-archlinux">Install driver (archlinux)</h3>
<hr/>
<ul>
<li><a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units</a></li>
<li><a href="https://en.wikipedia.org/wiki/OpenCL">https://en.wikipedia.org/wiki/OpenCL</a></li>
<li><a href="https://wiki.archlinux.org/title/GPGPU">https://wiki.archlinux.org/title/GPGPU</a></li>
</ul>
<p>Run</p>
<div class="remark-highlight"><pre class="language-bash"><code class="language-bash">$ lspci <span class="token parameter variable">-k</span> <span class="token parameter variable">-d</span> ::03xx
01:00.0 VGA compatible controller: NVIDIA Corporation GP104 <span class="token punctuation">[</span>GeForce GTX <span class="token number">1070</span><span class="token punctuation">]</span> <span class="token punctuation">(</span>rev a1<span class="token punctuation">)</span>
	Subsystem: ASUSTeK Computer Inc. Device <span class="token number">8599</span>
	Kernel driver <span class="token keyword">in</span> use: nouveau
	Kernel modules: nouveau, nvidia_drm, nvidia
</code></pre></div>
<p>Look for your name:</p>
<p><a href="https://nouveau.freedesktop.org/CodeNames.html">https://nouveau.freedesktop.org/CodeNames.html</a></p>
<p>Install correct package according to:
<a href="https://wiki.archlinux.org/title/NVIDIA#Installation">https://wiki.archlinux.org/title/NVIDIA<!-- -->https://wiki.archlinux.org/title/NVIDIA#Installation</a></p>
<p>Restart machine</p>
<div class="remark-highlight"><pre class="language-unknown"><code class="language-unknown">nvidia-smi</code></pre></div>
<p>Install cuda</p>
<p>Verify installation</p>
<div class="remark-highlight"><pre class="language-unknown"><code class="language-unknown">nvcc --version
</code></pre></div>
<ul>
<li>nvidia</li>
<li>nvidia-utils: provides nvidia-libgl, opengl-driver, vulkan-driver</li>
<li>cuda</li>
<li>opencl-nvidia</li>
</ul>
<h3 id="python-and-cuda">Python and CUDA</h3>
<hr/>
<ul>
<li>PyCUDA</li>
<li>Numba</li>
</ul>
<h3 id="numba">Numba</h3>
<hr/>
<blockquote>
<p>Numba is an open source, NumPy-aware optimizing compiler for Python sponsored by Anaconda, Inc. It uses the LLVM compiler project to generate machine code from Python syntax.
Numba can compile a large subset of numerically-focused Python, including many NumPy functions. Additionally, Numba has support for automatic parallelization of loops, generation of GPU-accelerated code, and creation of ufuncs and C callbacks.</p>
</blockquote>
<ul>
<li>Numba: <a href="https://numba.pydata.org/">https://numba.pydata.org/</a></li>
<li>Numba github: <a href="https://github.com/numba/numba">https://github.com/numba/numba</a></li>
<li><a href="https://numba.readthedocs.io/en/stable/cuda/index.html">https://numba.readthedocs.io/en/stable/cuda/index.html</a></li>
</ul>
<h2 id="define">Define</h2>
<hr/>
<ul>
<li>SM</li>
<li>CUDA programming model</li>
</ul>
<h2 id="resources">Resources</h2>
<hr/>
<ul>
<li><a href="https://developer.nvidia.com/blog/tag/cuda-refresher/">https://developer.nvidia.com/blog/tag/cuda-refresher/</a></li>
<li><a href="https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/">https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/</a></li>
<li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/">https://docs.nvidia.com/cuda/cuda-c-programming-guide/</a></li>
<li><a href="https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf">https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf</a></li>
<li><a href="https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf">https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf</a></li>
<li>cuda in python part1: <a href="https://www.vincent-lunot.com/post/an-introduction-to-cuda-in-python-part-1/">https://www.vincent-lunot.com/post/an-introduction-to-cuda-in-python-part-1/</a></li>
<li>cuda compatibility: <a href="https://developer.nvidia.com/cuda-gpus">https://developer.nvidia.com/cuda-gpus</a></li>
<li>nvidia hpc: <a href="https://developer.nvidia.com/hpc">https://developer.nvidia.com/hpc</a></li>
<li>archlinux gpgpu: <a href="https://wiki.archlinux.org/title/GPGPU">https://wiki.archlinux.org/title/GPGPU</a></li>
<li>cuda python: <a href="https://developer.nvidia.com/cuda-python">https://developer.nvidia.com/cuda-python</a></li>
<li>cuda python doc: <a href="https://nvidia.github.io/cuda-python/latest/">https://nvidia.github.io/cuda-python/latest/</a></li>
<li>cuda python github: <a href="https://github.com/NVIDIA/cuda-python/tree/main">https://github.com/NVIDIA/cuda-python/tree/main</a></li>
<li>opencl: <a href="https://www.khronos.org/opencl/#ocl-overview">https://www.khronos.org/opencl/<!-- -->https://www.khronos.org/opencl/#ocl-overview</a></li>
<li>archlinux nvidia: <a href="https://wiki.archlinux.org/title/NVIDIA">https://wiki.archlinux.org/title/NVIDIA</a></li>
<li>nvidia codenames: <a href="https://nouveau.freedesktop.org/CodeNames.html">https://nouveau.freedesktop.org/CodeNames.html</a></li>
</ul>
<p>old</p>
<ul>
<li>Leetcpu: <a href="https://www.leetgpu.com/resources">https://www.leetgpu.com/resources</a></li>
<li>Really nice glossary: <a href="https://modal.com/gpu-glossary">https://modal.com/gpu-glossary</a></li>
<li>CUDA programming guide: <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/">https://docs.nvidia.com/cuda/cuda-c-programming-guide/</a></li>
<li>CUDA books archive: <a href="https://developer.nvidia.com/cuda-books-archive">https://developer.nvidia.com/cuda-books-archive</a></li>
<li>CUDA parallel programming: <a href="https://newfrontiers.illinois.edu/news-and-events/introduction-to-parallel-programming-with-cuda/">https://newfrontiers.illinois.edu/news-and-events/introduction-to-parallel-programming-with-cuda/</a></li>
<li>Triton: <a href="https://triton-lang.org/main/getting-started/tutorials/index.html">https://triton-lang.org/main/getting-started/tutorials/index.html</a></li>
<li>Pytorch doc: <a href="https://docs.pytorch.org/docs/stable/index.html">https://docs.pytorch.org/docs/stable/index.html</a></li>
<li>Pytorch tutorials: <a href="https://docs.pytorch.org/tutorials/">https://docs.pytorch.org/tutorials/</a></li>
<li>Pytorch zero to mastery: <a href="https://www.learnpytorch.io/">https://www.learnpytorch.io/</a></li>
<li>Tinygrad doc: <a href="https://docs.tinygrad.org/">https://docs.tinygrad.org/</a></li>
<li>Tinygrad notes: <a href="https://mesozoic-egg.github.io/tinygrad-notes/">https://mesozoic-egg.github.io/tinygrad-notes/</a></li>
<li>Mojo: <a href="https://docs.modular.com/mojo/manual/">https://docs.modular.com/mojo/manual/</a></li>
</ul></div></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/hpc","query":{},"buildId":"aa3v2nN24CrFndNCoWAn5","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>